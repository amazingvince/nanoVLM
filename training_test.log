--- VLM Config ---
VLMConfig(vit_architecture='dinov3', vit_model_type='facebook/dinov3-vits16plus-pretrain-lvd1689m', vit_hidden_dim=768, vit_inter_dim=3072, vit_patch_size=16, vit_img_size=224, vit_n_heads=12, vit_dropout=0.0, vit_n_blocks=12, vit_ln_eps=1e-06, vit_cls_flag=True, vit_num_registers=4, vit_use_swiglu=True, vit_use_rope=False, vit_use_sincos_pos=True, vit_rope_base=10000.0, vit_layer_scale=True, vit_layer_scale_init=1.0, vit_drop_path_rate=0.0, lm_architecture='gemma', lm_model_type='google/gemma-3-270m-it', lm_tokenizer='google/gemma-3-270m-it', lm_hidden_dim=960, lm_inter_dim=2560, lm_rms_eps=1e-05, lm_re_base=100000, lm_max_position_embeddings=8192, lm_base_vocab_size=262144, extra_token_amount=17, lm_vocab_size=262161, lm_n_heads=15, lm_n_kv_heads=5, lm_dropout=0.0, lm_n_blocks=32, lm_attn_scaling=1.0, lm_max_length=1024, lm_use_tokens=False, lm_tie_weights=True, lm_chat_template="{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}", mp_pixel_shuffle_factor=2, mp_image_token_length=49, mp_handle_special_tokens=True, max_img_size=1024, vlm_extra_tokens={'image_token': '<|image|>', 'r1c1': '<row_1_col_1>', 'r1c2': '<row_1_col_2>', 'r1c3': '<row_1_col_3>', 'r1c4': '<row_1_col_4>', 'r2c1': '<row_2_col_1>', 'r2c2': '<row_2_col_2>', 'r2c3': '<row_2_col_3>', 'r2c4': '<row_2_col_4>', 'r3c1': '<row_3_col_1>', 'r3c2': '<row_3_col_2>', 'r3c3': '<row_3_col_3>', 'r3c4': '<row_3_col_4>', 'r4c1': '<row_4_col_1>', 'r4c2': '<row_4_col_2>', 'r4c3': '<row_4_col_3>', 'r4c4': '<row_4_col_4>'}, vlm_load_backbone_weights=True, vlm_checkpoint_path='checkpoints', hf_repo_name='nanoVLM')
--- Train Config ---
TrainConfig(lr_mp=0.00512, lr_backbones=5e-05, data_cutoff_idx=None, val_ratio=0.025, batch_size=2, gradient_accumulation_steps=32, max_grad_norm=1.0, eval_in_epochs=True, eval_interval=3200, stats_log_interval=800, max_training_steps=5000, max_images_per_example=4, max_images_per_knapsack=18, max_sample_length=1024, compile=True, resume_from_vlm_checkpoint=False, train_dataset_path='HuggingFaceM4/the_cauldron', train_dataset_name=('all',), wandb_entity=None, log_wandb=False, use_lmms_eval=True, lmms_eval_tasks='mmstar,mmmu,ocrbench,textvqa', lmms_eval_limit=2000, lmms_eval_batch_size=128, num_workers=1, max_threads=2)
Warning: Failed to load dataset config 'clevr_math' from 'HuggingFaceM4/the_cauldron'. Error: [Errno 2] No such file or directory: '/fsx/m4/datasets/downloads/extracted/3c4c03ad359586cd332583e3a61e1ef5808cc52f30cef52648847fd19d477eac/CLEVR_v1.0/images/train/CLEVR_train_000000.png'
Warning: Failed to load dataset config 'okvqa' from 'HuggingFaceM4/the_cauldron'. Error: [Errno 2] No such file or directory: '/fsx/m4/datasets/downloads/extracted/19661dd042ca9f1e30d4843440822fb38f18fc5d649662da48018561ddec94e2/train2014/COCO_train2014_000000051606.jpg'
Loading from backbone weights
Successfully loaded facebook/dinov3-vits16plus-pretrain-lvd1689m weights from safetensors. Model has 28,697,088 parameters.
Extending token embeddings from torch.Size([262144, 640]) to torch.Size([262161, 640])
Initialized 17 new token embeddings
Successfully loaded google/gemma-3-270m-it weights from safetensors. Model has 268,076,800 parameters.
nanoVLM initialized with 297,756,928 parameters
Training summary: 423791 samples, 211895 batches/epoch, batch size 64
Validation summary: 45049 samples, 22524 batches/epoch, batch size 64
Using device: cuda
Exception in thread Thread-3 (_producer):
Traceback (most recent call last):
  File "/home/pszemraj/miniforge3/envs/vlm/lib/python3.12/threading.py", line 1075, in _bootstrap_inner
    self.run()
  File "/home/pszemraj/miniforge3/envs/vlm/lib/python3.12/threading.py", line 1012, in run
    self._target(*self._args, **self._kwargs)
  File "/home/pszemraj/workspace/projects/vlm/nanoVLM/data/advanced_datasets.py", line 110, in _producer
    sample = next(iterator)
             ^^^^^^^^^^^^^^
  File "/home/pszemraj/workspace/projects/vlm/nanoVLM/data/advanced_datasets.py", line 79, in sharded_item_iterator
    yield self.dataset[idx]
          ~~~~~~~~~~~~^^^^^
  File "/home/pszemraj/workspace/projects/vlm/nanoVLM/data/datasets.py", line 93, in __getitem__
    messages = self._get_messages(item, splitted_image_counts)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pszemraj/workspace/projects/vlm/nanoVLM/data/datasets.py", line 32, in _get_messages
    image_string = get_image_string(self.tokenizer, splitted_image_counts, self.mp_image_token_length)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pszemraj/workspace/projects/vlm/nanoVLM/data/processors.py", line 81, in get_image_string
    image_string += getattr(tokenizer, f"r{i + 1}c{j + 1}")
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pszemraj/miniforge3/envs/vlm/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 1099, in __getattr__
    raise AttributeError(f"{self.__class__.__name__} has no attribute {key}")
AttributeError: GemmaTokenizerFast has no attribute r1c5. Did you mean: 'r1c1'?
