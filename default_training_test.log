--- VLM Config ---
VLMConfig(vit_architecture='siglip', vit_model_type='google/siglip2-base-patch16-512', vit_hidden_dim=768, vit_inter_dim=3072, vit_patch_size=16, vit_img_size=512, vit_channels=3, vit_n_heads=12, vit_dropout=0.0, vit_n_blocks=12, vit_ln_eps=1e-06, vit_cls_flag=False, vit_num_registers=0, vit_use_swiglu=False, vit_use_rope=False, vit_use_sincos_pos=False, vit_rope_base=10000.0, vit_layer_scale=False, vit_layer_scale_init=1.0, vit_drop_path_rate=0.0, vit_rope_augment=True, lm_architecture='llama', lm_model_type='HuggingFaceTB/SmolLM2-360M-Instruct', lm_tokenizer='HuggingFaceTB/SmolLM2-360M-Instruct', lm_hidden_dim=960, lm_inter_dim=2560, lm_rms_eps=1e-05, lm_re_base=100000, lm_max_position_embeddings=8192, lm_base_vocab_size=49152, extra_token_amount=17, lm_vocab_size=49280, lm_n_heads=15, lm_n_kv_heads=5, lm_dropout=0.0, lm_n_blocks=32, lm_attn_scaling=1.0, lm_max_length=1024, lm_use_tokens=False, lm_tie_weights=True, lm_chat_template="{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}", mp_pixel_shuffle_factor=4, mp_image_token_length=64, mp_handle_special_tokens=False, max_img_size=1024, vlm_extra_tokens={'image_token': '<|image|>', 'r1c1': '<row_1_col_1>', 'r1c2': '<row_1_col_2>', 'r1c3': '<row_1_col_3>', 'r1c4': '<row_1_col_4>', 'r2c1': '<row_2_col_1>', 'r2c2': '<row_2_col_2>', 'r2c3': '<row_2_col_3>', 'r2c4': '<row_2_col_4>', 'r3c1': '<row_3_col_1>', 'r3c2': '<row_3_col_2>', 'r3c3': '<row_3_col_3>', 'r3c4': '<row_3_col_4>', 'r4c1': '<row_4_col_1>', 'r4c2': '<row_4_col_2>', 'r4c3': '<row_4_col_3>', 'r4c4': '<row_4_col_4>'}, vlm_load_backbone_weights=True, vlm_checkpoint_path='checkpoints', hf_repo_name='nanoVLM')
--- Train Config ---
TrainConfig(lr_mp=0.00512, lr_backbones=5e-05, data_cutoff_idx=None, val_ratio=0.025, batch_size=2, gradient_accumulation_steps=32, max_grad_norm=1.0, eval_in_epochs=True, eval_interval=3200, stats_log_interval=800, console_log_interval=10, max_training_steps=5000, max_images_per_example=4, max_images_per_knapsack=18, max_sample_length=1024, compile=False, resume_from_vlm_checkpoint=False, freeze_vision_encoder=False, train_dataset_path='HuggingFaceM4/the_cauldron', train_dataset_name=('all',), wandb_entity=None, log_wandb=False, wandb_log_steps=5, use_lmms_eval=True, lmms_eval_tasks='mmstar,mmmu,ocrbench,textvqa', lmms_eval_limit=2000, lmms_eval_batch_size=128, num_workers=2, max_threads=4)
Warning: Failed to load dataset config 'clevr_math' from 'HuggingFaceM4/the_cauldron'. Error: [Errno 2] No such file or directory: '/fsx/m4/datasets/downloads/extracted/3c4c03ad359586cd332583e3a61e1ef5808cc52f30cef52648847fd19d477eac/CLEVR_v1.0/images/train/CLEVR_train_000000.png'
Warning: Failed to load dataset config 'okvqa' from 'HuggingFaceM4/the_cauldron'. Error: [Errno 2] No such file or directory: '/fsx/m4/datasets/downloads/extracted/19661dd042ca9f1e30d4843440822fb38f18fc5d649662da48018561ddec94e2/train2014/COCO_train2014_000000051606.jpg'
Loading from backbone weights
Successfully loaded google/siglip2-base-patch16-512 weights from safetensors. Model has 86,433,024 parameters.
Extending token embeddings from torch.Size([49152, 960]) to torch.Size([49280, 960])
Initialized 128 new token embeddings
Successfully loaded HuggingFaceTB/SmolLM2-360M-Instruct weights from safetensors. Model has 361,944,000 parameters.
nanoVLM initialized with 460,173,504 parameters
Training summary: 449528 samples, 224764 batches/epoch, batch size 64
Validation summary: 45049 samples, 22524 batches/epoch, batch size 64
Using device: cuda
Token indices sequence length is longer than the specified maximum sequence length for this model (26722 > 8192). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (8514 > 8192). Running this sequence through the model will result in indexing errors
